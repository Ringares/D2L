{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training process\n",
    "    - defining neural network architectures\n",
    "    - handing data\n",
    "    - specifying loss function: a measure of ftness\n",
    "    - training the model\n",
    "- monitoring\n",
    "- save and load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression 的解析解法\n",
    "\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b)=\\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}\n",
    "$$\n",
    "$$\n",
    "L(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)^{2}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{w}^{*}, b^{*}=\\underset{\\mathbf{w}, b}{\\operatorname{argmin}} L(\\mathbf{w}, b)\n",
    "$$\n",
    "因为线性回归的形式比较简单, 是凸优化问题,\" 其损失函数是严格的凸函数, 有唯一的全局最优解. 通过计算梯度为0, 我们可以求得参数 W 的最优解 (bias 可以通过扩展 x 融入 W 参数中), 是损失函数最小.\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{*}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降解法\n",
    "\n",
    "当我们面对高维和非凸损失函数的时候, 我们还可以用梯度下降的方法, 有效的训练我们的模型. 在凸损失面上, 梯度下降算法使我们最终能到达全局最优点; 而对于非凸损失面来说, 我们也能到达相对较好的局部最优点."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\mathbf{w}, b) \\leftarrow(\\mathbf{w}, b)-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w}, b)} l^{(i)}(\\mathbf{w}, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression 的 Squared Loss 对应的噪声假设"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import d2l_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# torch.cuda.manual_seed_all(0)\n",
    "\n",
    "torch.__version__               # PyTorch version\n",
    "# torch.version.cuda              # Corresponding CUDA version\n",
    "# torch.backends.cudnn.version()  # Corresponding cuDNN version\n",
    "# torch.cuda.get_device_name(0)   # GPU type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_true = 4.2\n",
    "W_true = np.array([2, -3.4])\n",
    "X, y = d2l_utils.synthetic_data(W_true, b_true, num_examples=1000)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_set(bs):\n",
    "    data_size = len(X)\n",
    "    # random index\n",
    "    index = list(range(data_size))\n",
    "    np.random.shuffle(index)\n",
    "    for i in range(int(data_size/bs)):\n",
    "        batch_index = index[i*bs:(i+1)*bs]\n",
    "        yield X[batch_index], y[batch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11755205  0.63501331]\n",
      " [-0.2961268  -0.24404977]\n",
      " [-1.02658859  0.0330473 ]\n",
      " [-0.68634894 -0.41995842]\n",
      " [ 0.16157396 -1.08579457]\n",
      " [-1.30434874  0.72427993]\n",
      " [ 0.78794457  0.1134702 ]\n",
      " [ 0.17103901 -0.39185839]\n",
      " [-0.4424496   0.57908332]\n",
      " [-0.28069055  0.87528349]] \n",
      " [ 2.28279008  4.42703311  2.04263248  4.24611958  8.20873184 -0.87228472\n",
      "  5.39126725  5.87539651  1.36629728  0.65764181]\n"
     ]
    }
   ],
   "source": [
    "for feats, labels in data_set(bs=10): \n",
    "    print(feats, '\\n', labels) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy 实现 解析解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.concatenate([np.ones((1000,1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.20028744,  1.99997122, -3.4002213 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X_.T@X_)@(X_.T)@y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy 实现 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(feats, W, b):\n",
    "    return feats@W+b\n",
    "\n",
    "def square_loss(y_hat, label):\n",
    "    return np.sum((y_hat-label)**2)/2\n",
    "\n",
    "def cal_grad(X, y_hat, label):\n",
    "    return X.T@(y_hat-label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.19998571]), array([ 1.99993809, -3.3999418 ]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "W = np.random.randn(2)\n",
    "b = np.zeros(1)\n",
    "epoch = 50\n",
    "bs = 48\n",
    "lr = 1e-2\n",
    "\n",
    "for _ in range(epoch):\n",
    "    for X_batch, y_batch in data_set(bs):\n",
    "        y_hat_batch = linear_regression(X_batch, W, b)\n",
    "        W -= lr*cal_grad(X_batch, y_hat_batch, y_batch)/bs\n",
    "        b -= lr*cal_grad(np.ones(len(X_batch)), y_hat_batch, y_batch)/bs\n",
    "    #print('loss: %s' % square_loss(y_hat_batch, y_batch))\n",
    "\n",
    "b, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch 自动求导 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.2003], requires_grad=True), tensor([[ 2.0000],\n",
       "         [-3.4002]], requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((2,1), dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "epoch = 50\n",
    "bs = 48\n",
    "lr = 1e-2\n",
    "loss = torch.nn.MSELoss() # lambda y1,y2: ((y1-y2)**2).mean()\n",
    "\n",
    "def linreg(inputs, W, b):\n",
    "    return torch.mm(inputs, W) + b\n",
    "\n",
    "for _ in range(epoch):\n",
    "    for X_batch, labels in data_set(bs):\n",
    "        X_batch = torch.tensor(X_batch, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            \n",
    "        output = linreg(X_batch, W, b)\n",
    "        l = loss(output.reshape(labels.shape), labels)\n",
    "        \n",
    "        l.backward()\n",
    "        W.data -= lr*W.grad\n",
    "        b.data -= lr*b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    #print('loss: %s' % loss(output, labels).mean().item())\n",
    "b, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch API 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_true = 4.2\n",
    "W_true = np.array([2, -3.4])\n",
    "X, y = d2l_utils.synthetic_data(W_true, b_true, num_examples=1000)\n",
    "\n",
    "batch_size = 10\n",
    "data_set = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X).float(),\n",
    "    torch.tensor(y).float()\n",
    ")\n",
    "data_iter = torch.utils.data.DataLoader(data_set, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.3773,  0.1604],\n",
      "        [ 0.4788,  1.2376],\n",
      "        [-0.4982,  0.9055],\n",
      "        [-1.5247, -0.5055],\n",
      "        [ 0.8241,  0.7203],\n",
      "        [ 0.2672,  0.3321],\n",
      "        [-1.2725,  1.3932],\n",
      "        [ 0.8224, -0.9617],\n",
      "        [-1.5391, -0.5665],\n",
      "        [-0.4379, -0.2326]]), tensor([ 4.4158,  0.9411,  0.1151,  2.8879,  3.4022,  3.6053, -3.0696,  9.1115,\n",
      "         3.0506,  4.1171])]\n"
     ]
    }
   ],
   "source": [
    "for i in data_iter:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "w = torch.empty((2,1), dtype=torch.float32, requires_grad=True)\n",
    "b = torch.empty(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "class LinearNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        torch.nn.init.normal_(self.linear.weight, 0., 0.1)\n",
    "        torch.nn.init.constant_(self.linear.bias, 0.)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "net = LinearNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1488, 0.0219]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSELoss()\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.03\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "print(loss)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.03, )\n",
    "print(optimizer)\n",
    "\n",
    "# optimizer =optim.SGD([\n",
    "#     # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "#     {'params': net.subnet1.parameters()}, # lr=0.03\n",
    "#     {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "# ], lr=0.03)net.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000484\n",
      "epoch 2, loss: 0.000242\n",
      "epoch 3, loss: 0.000166\n",
      "epoch 4, loss: 0.000543\n",
      "epoch 5, loss: 0.000533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9927, -3.3844]]), tensor([4.1993]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(output.shape))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))\n",
    "net.linear.weight.data, net.linear.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "epoch 1, loss: 0.000368\n",
      "epoch 2, loss: 0.000483\n",
      "epoch 3, loss: 0.000791\n",
      "epoch 4, loss: 0.000635\n",
      "epoch 5, loss: 0.000776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9917, -3.3824]]), tensor([4.1985]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LinearNet() # initiallized\n",
    "print(net)\n",
    "\n",
    "optimizer_w = torch.optim.SGD([net.linear.weight], lr=0.03, weight_decay=0.01)\n",
    "optimizer_b = torch.optim.SGD([net.linear.bias], lr=0.03)\n",
    "\n",
    "num_epochs = 5\n",
    "optimizers = [optimizer_w, optimizer_b]\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for X, y in data_iter:\n",
    "        for opti in optimizers:\n",
    "            opti.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y.view(y_hat.shape))\n",
    "        l.backward()\n",
    "        \n",
    "        for opti in optimizers:\n",
    "            opti.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))   \n",
    "net.linear.weight.data, net.linear.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_torch_kernel",
   "language": "python",
   "name": "py3_torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
